# Machine Translation Recipe - Example Walkthrough

This document demonstrates the machine translation recipe with concrete examples showing input data, prompt formatting, training, inference, and evaluation.

## ğŸ“ Sample Data

Our sample dataset (`sample_data.csv`):
```csv
source,target
"Hello, how are you today?","Hola, Â¿cÃ³mo estÃ¡s hoy?"
"I would like to order a coffee, please.","Me gustarÃ­a pedir un cafÃ©, por favor."
"The weather is beautiful today.","El clima estÃ¡ hermoso hoy."
"Can you help me find the train station?","Â¿Puedes ayudarme a encontrar la estaciÃ³n de tren?"
"I'm learning Spanish to travel to South America.","Estoy aprendiendo espaÃ±ol para viajar a SudamÃ©rica."
```

## ğŸ”„ Data Processing

### Input Processing
The recipe automatically converts CSV data into instruction-following format:

**Original Data:**
- Source: "Hello, how are you today?"
- Target: "Hola, Â¿cÃ³mo estÃ¡s hoy?"

**Processed Training Example:**
```
<|im_start|>system
You are a professional translator with expertise in English and Spanish. Provide accurate, fluent, and contextually appropriate translations.
<|im_end|>
<|im_start|>user
Translate the following text from English to Spanish:

Hello, how are you today?
<|im_end|>
<|im_start|>assistant
Hola, Â¿cÃ³mo estÃ¡s hoy?<|im_end|>
```

## ğŸ¯ Domain-Specific Examples

### General Domain
```
Instruction: "Translate the following text from English to Spanish:"
Example: "Good morning" â†’ "Buenos dÃ­as"
```

### Technical Domain
```
Instruction: "Translate the following technical text from English to Spanish, preserving technical terminology:"
Example: "The API endpoint returns JSON data" â†’ "El endpoint de la API devuelve datos JSON"
```

### Medical Domain
```
Instruction: "Translate the following medical text from English to Spanish, maintaining medical accuracy:"
Example: "The patient shows symptoms of inflammation" â†’ "El paciente muestra sÃ­ntomas de inflamaciÃ³n"
```

### Business Domain
```
Instruction: "Translate the following business text from English to Spanish:"
Example: "Please schedule a meeting for tomorrow" â†’ "Por favor, programe una reuniÃ³n para maÃ±ana"
```

## ğŸ‹ï¸ Training Configuration

### LoRA Training (Recommended)
```yaml
# configs/mt_lora.yaml
model_name_or_path: openai/gpt-oss-20b
source_lang: en
target_lang: es
use_peft: true
lora_r: 16
lora_alpha: 32
learning_rate: 1.0e-4
num_train_epochs: 5
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
max_length: 1024
```

### Training Command
```bash
python machine_translation.py \
    --config configs/mt_lora.yaml \
    --source_lang en \
    --target_lang es \
    --dataset_name csv \
    --dataset_config data_files=sample_data.csv \
    --run_name en-es-translator
```

### Expected Training Progress
```
Epoch 1/5: Loss = 2.45, BLEU = 15.2
Epoch 2/5: Loss = 1.89, BLEU = 22.7
Epoch 3/5: Loss = 1.52, BLEU = 28.1
Epoch 4/5: Loss = 1.31, BLEU = 31.5
Epoch 5/5: Loss = 1.18, BLEU = 33.8
```

## ğŸ”® Inference Examples

### Single Text Translation
```bash
python generate_translation.py \
    --model_path ./gpt-oss-20b-translator-lora \
    --source_lang en \
    --target_lang es \
    --text "Good morning, how can I help you?"
```

**Output:**
```
Translation: Buenos dÃ­as, Â¿cÃ³mo puedo ayudarte?
```

### Batch Translation
**Input file (test_input.txt):**
```
Good morning, how can I help you?
The meeting is scheduled for 3 PM today.
I love reading books in my free time.
Could you please repeat that question?
The new restaurant downtown has excellent reviews.
```

**Command:**
```bash
python generate_translation.py \
    --model_path ./gpt-oss-20b-translator-lora \
    --source_lang en \
    --target_lang es \
    --input_file test_input.txt \
    --output_file translations.txt
```

**Output (translations.txt):**
```
Buenos dÃ­as, Â¿cÃ³mo puedo ayudarte?
La reuniÃ³n estÃ¡ programada para las 3 PM hoy.
Me encanta leer libros en mi tiempo libre.
Â¿PodrÃ­as repetir esa pregunta, por favor?
El nuevo restaurante del centro tiene excelentes reseÃ±as.
```

## ğŸ“Š Evaluation

### Evaluation Command
```bash
python evaluate_translation.py \
    --model_path ./gpt-oss-20b-translator-lora \
    --source_lang en \
    --target_lang es \
    --test_file sample_data.csv \
    --source_column source \
    --target_column target \
    --output_file evaluation_results.json
```

### Sample Evaluation Results
```json
{
  "num_examples": 10,
  "generation_time": 45.2,
  "avg_time_per_example": 4.52,
  "source_lang": "en",
  "target_lang": "es",
  "bleu": 34.8,
  "chrf": 58.2,
  "ter": 42.1,
  "comet": 0.742,
  "avg_pred_length": 8.5,
  "avg_ref_length": 8.1,
  "length_ratio": 1.049,
  "examples": [
    {
      "source": "Hello, how are you today?",
      "reference": "Hola, Â¿cÃ³mo estÃ¡s hoy?",
      "prediction": "Hola, Â¿cÃ³mo estÃ¡s hoy?"
    }
  ]
}
```

## ğŸŒ Language Pair Examples

### English â†’ Spanish
```
EN: "Thank you for your help"
ES: "Gracias por tu ayuda"
```

### English â†’ French
```
EN: "Thank you for your help"
FR: "Merci pour votre aide"
```

### English â†’ German
```
EN: "Thank you for your help"
DE: "Danke fÃ¼r Ihre Hilfe"
```

### English â†’ Japanese
```
EN: "Thank you for your help"
JA: "ã”å”åŠ›ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™"
```

## ğŸ“ˆ Performance Metrics

### Training Performance
- **LoRA Training**: ~2-3 hours on A6000 GPU
- **Full Fine-tuning**: ~8-10 hours on A100 GPU
- **Memory Usage**: 16GB (LoRA) vs 40GB (Full)

### Translation Quality
- **BLEU Score**: 25-40 (depending on language pair and domain)
- **COMET Score**: 0.65-0.85 (neural metric)
- **Translation Speed**: ~2-5 seconds per sentence

### Hardware Requirements
- **Minimum**: RTX 4090 24GB (LoRA training)
- **Recommended**: A6000 48GB (LoRA) or A100 80GB (Full)
- **Inference**: Any GPU with 8GB+ VRAM

## ğŸ› ï¸ Configuration Options

### Model Sizes
```python
# 20B model (default)
model_name_or_path: "openai/gpt-oss-20b"

# 120B model (requires multiple GPUs)
model_name_or_path: "openai/gpt-oss-120b"
```

### Generation Parameters
```python
# Conservative (more accurate)
temperature: 0.1
top_p: 0.9
repetition_penalty: 1.1

# Balanced (default)
temperature: 0.3
top_p: 0.9
repetition_penalty: 1.1

# Creative (more varied)
temperature: 0.7
top_p: 0.95
repetition_penalty: 1.0
```

## ğŸ“‹ Supported Language Pairs

| Source | Target | Status | BLEU Score Range |
|--------|--------|--------|------------------|
| English | Spanish | âœ… | 30-40 |
| English | French | âœ… | 28-38 |
| English | German | âœ… | 25-35 |
| English | Italian | âœ… | 30-40 |
| English | Portuguese | âœ… | 32-42 |
| English | Russian | âœ… | 20-30 |
| English | Japanese | âœ… | 18-28 |
| English | Korean | âœ… | 15-25 |
| English | Chinese | âœ… | 20-30 |
| English | Dutch | âœ… | 28-38 |
| English | Polish | âœ… | 22-32 |
| English | Ukrainian | âœ… | 20-30 |

## ğŸš€ Quick Start Workflow

### 1. Prepare Data
```bash
# Create CSV with source,target columns
echo "source,target" > my_data.csv
echo "\"Hello\",\"Hola\"" >> my_data.csv
```

### 2. Train Model
```bash
python machine_translation.py \
    --config configs/mt_lora.yaml \
    --dataset_name csv \
    --dataset_config data_files=my_data.csv
```

### 3. Test Translation
```bash
python generate_translation.py \
    --model_path ./gpt-oss-20b-translator-lora \
    --text "How are you?"
```

### 4. Evaluate Performance
```bash
python evaluate_translation.py \
    --model_path ./gpt-oss-20b-translator-lora \
    --test_file my_data.csv
```

## ğŸ¯ Best Practices

### Data Preparation
- âœ… Clean and normalize text
- âœ… Remove duplicate pairs
- âœ… Balance domain distribution
- âœ… Include evaluation set

### Training Tips
- ğŸ¯ Start with LoRA for experimentation
- ğŸ¯ Use domain-specific data when possible
- ğŸ¯ Monitor BLEU scores during training
- ğŸ¯ Save checkpoints regularly

### Evaluation Guidelines
- ğŸ“Š Use multiple metrics (BLEU, COMET, chrF)
- ğŸ“Š Test on held-out data
- ğŸ“Š Include human evaluation for quality
- ğŸ“Š Check different domains separately

## ğŸ”§ Troubleshooting

### Common Issues
1. **Out of Memory**: Reduce batch size, use LoRA, enable gradient checkpointing
2. **Poor Quality**: More training data, domain-specific data, adjust learning rate
3. **Slow Training**: Use Flash Attention, increase batch size with more GPUs

### Performance Optimization
- Use `attn_implementation: kernels-community/vllm-flash-attn3`
- Enable gradient checkpointing for memory efficiency
- Use mixed precision training (automatic in recipe)
- Optimize data loading with multiple workers

This walkthrough demonstrates the complete machine translation pipeline from data preparation to evaluation, showing how the GPT-OSS recipe can be used for high-quality neural machine translation. 